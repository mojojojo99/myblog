---
title: "The happy little accidents"
last_modified_at: 2020-05-21T16:20:02-05:00
excerpt: "### Part II of Operating Systems (15-410) and my experience <br/> <br/> <br/>"
categories:
  - Coding
tags:
  - Operating Systems
header:
  teaser: /assets/images/post2.jpg
  overlay_image: /assets/images/post2.jpg
  overlay_filter: 0.5
---

### Part II of Operating Systems (15-410) and my experience

<br>

*Hmmm... That's not right...Uh-oh* 

<br>
 --- The so-called three types of errors 


<br>

In what you might call a deliberate rashness, my partner and I took the OS class (15-410) class, overloading it together with other CS classes like 15-451 and 15-259 (and other classes) in sophomore spring. 

This continues the previous post with some of the design decisions we made for our kernel project. More technical learning points will be deferred to yet probably another post. (There are simply too many lessons learnt so other lessons will have to wait for perhaps a third post)

<br>

<center>............................................</center>


<br>

#### Interesting Detours

I'd like to talk about some of our thought processes when we designed our kernel. These design decisions may not be the best, but I think it's perhaps comic to document some of our imperfect approaches in a time capsule, dig it up a few years down the road and joke at the folly things we do.

<br>

##### Thread Control Blocks

One of the core abstractions of the kernel is the thread and process concepts. Although it seems pretty straightforward how a layer of indirection (threads and processes) can easily help to bridge the gaps between hardware and user, I think it's already applaudable how we arrived at these clean and neat idea of "task" abstraction that exactly nailed down the exact idea of a "thing" to manage space, time slices, resources etc. 

Standing the shoulders of giants, we now *just* have to find a way to implement and realise these abstractions.

Firstly, we used a "thread control block" (tcb) to represent the concept of a "kernel thread" and a "process control block" (pcb) to represent the concept of a "kernel process". 

The blocks are used to store important identification information we need for each of the tasks like their ids, variable_queue links for traversal, current state information (blocked, running, runnable etc.). 

One seemingly elementary problem that we encountered was where to store these blocks. Might seem straightforward but after some thought, we decided to place this tcb struct in the kernel thread's stack and have the pcb structs malloc-ed. 

One main driving reason behind this is the use case of each of these control blocks. When a trap to kernel space is triggered, we have immediate access to the kernel thread stack or at least some semblance of where and hence who we are. If we can get access to our threads, we can simply store a pointer to the process block in each of the thread control blocks.

```
 _______________
|               | <- Top of kernel thread stack
|               |
|               |
|               |
|               |
|_______________|
|      tcb      |
|_______________| <- Bottom of kernel thread stack

```

Since we have that the thread stacks are all thread stack size aligned, we can easily find the tcb of the current kernel thread from the current thread's esp. This allows us to do gettid and other thread related syscalls, for example, in O(1) time. Besides, this also allows us to bypass having to malloc an additional structure size when we create each new kernel thread; since malloc is expensive, this is much more time efficient too. There is very little cost to this implementation since the same amount of space is used. One disadvantage of this is that this structure may easily be corrupted if the kernel threads used more than the allocated stack space. 

We decided on this design ultimately because this small disadvantage can be avoided by more careful use of the stack in kernel mode, in exchange for quite a huge improvement in time and space. To check that our current syscall implementations will stay within the stack space allocated, we also added canaries in the thread stack, and check frequently whether they are overwritten due to the overflowing stack.

<br>

##### Process wait and exit

Another one of the more tricky designs was with regards to the process wait and exit. When a child exits, we want a parent either currently waiting on the child, or a parent who calls on wait in future to be able to "reap" the child and get its exit information etc. Note that the parent process might have multiple threads so it should be possible for each of the parent threads to wait for the children. 

The fact that multiple parents can be waiting on multiple children in a multi-threaded environment suggests that some concurrency structures must be in place to prevent race conditions. 

###### One misguided initial idea

This problem really taught me how sometimes taking a step back and revisiting the problem another time can really help ---

Initially, we came up with a really horribly complicated design: 
- We have a list of parents in the parent process block, each with a waiting status flag 
- When a parent waits, we set this waiting status flag correspondingly
- When a child is exiting, we looped through the list of parent threads and change exactly one parent's waiting status to mark that it has reaped a child

There are so many things wrong with this design. 

- One glaring problem is how we have to hold a lock, and loop through all the parent threads which could take up to O(N) time. 
- Furthermore, this lock is potentially highly contended for if the parent has many threads, which compounds the problem since each O(N) search means we are holding the lock for a long amount of time. 
- Besides, there are further complications with respect to updating the parent's waiting status with the child's exit information which requires us to hold on to possibly another lock while already holding on to the first lock.


Somehow, we reached enlightenment with a much simpler design after leaving it alone for some days and coming back to it after. 

- We have a list of waiting parent threads for each pcb struct. 
- In this case, we only have to try to pop one parent off the queue when the child is trying to exit. 

Not sure why this didn't occur to us from the start, but it is a very much neater and cleaner idea. 

- No holding of multiple locks while doing loops
- No complication when updating child statuses
- Simple O(1) pop front

I'm embarrassed.

But this I guess was a huge learning point for us: sometimes if we look down and get so caught up in doing one thing and fixing one thing, we lose sight of how perhaps this approach might not, afterall, be the best idea chosen. 

>There are many approaches to solving a problem
> <br>
> There are many *correct* approaches to solving a problem.
> <br>
> Pick the best of these *correct* approaches to solving a problem. 

Similar to what Dave (our prof) once said, a correct approach may not be the *right* approach that best fits the problem. 

Nailing down and hammering out one elaborate but unecessarily complex solution to a simple problem might not be ideal. 

Note to self: Take a step back sometimes. 


###### One sneaky little race condition

One more complication in our design of the wait and exit issue that kept causing pesky little race conditions. In our design above, we needed to lock on the parent waiting list. 

Our initial approach was to have a lock in each pcb, protecting this list of the waiting parents.

However, there was a race condition that would arise when both the parent and child are exiting. We needed a lock in the parent pcb for the waiting list, but this parent may be changing: Imagine a code sequence where we have

1. Child gets the parent
2. Parent exits and pcb is deallocated
3. Child tries to get the parent->waiting_list_lock

This may potentially result in a bad dereference. While possible careful use of volatile fields can solve this problem, we decided to use another approach to handle this case. 

We narrowed down the fundamental problem to this issue was that we required both parent->child and child->parent links, and having locks in any of the parent/ child process would present a problem similar to the race condition as mentioned above, because we would have to get the correct parent or child before getting the lock.

We approached this by introducing the process_group, which helps to represent the relationship between parent and child. In this case, both parent and child can reference the process group to get the lock, since the lock is not stored in either. We also place the zombie_list and the waiting parent list in this process group since both these queues will change when the parent child relation changes.

When a parent exits, its children also has to be reparented to the init process. This can also be done easily with the process group since we can simply change the parent field in the process group to the init group, and also append the two different queues together. We keep a reference count in the process group so that we know when to deallocate it when no child is referencing it.

Some semblance of such a "process group" seems to be present in linux and other operating systems too. It took us a lot of variations and rewriting of the code to finally reach this decision; and after this I feel like I start to appreciate how much work and effort is being put into designing kernel structures. 


##### COW (mooo)

One of the other big challenges we faced was definitely in implementing virtual memory. The big boss to this stage is definitely, without a doubt, introducing COW (although we might have introduced it a tad bit early -- premature optimization.). 

The idea behind Copy-On-Write (Cow) is simple: 

- When new processes are forked, they simply have their page table entries pointing to the same pages as the parent's, in a way implicitly "sharing" the pages. 
- Both parent's and child's pages are marked as read only. 
- If either process attempts to write to the shared pages, this will trigger a page fault (since the page is marked "read-only") and we can resolve the fault by copying and allocating a page on demand. 

The main reason for this is because typically most processes do not even modify any memory and are immediately called on to execute a new process for e.g. using exec, replacing its address space entirely. 

Using COW hence saves the work of copying all pages, by doing a form of "lazy" copy.


In our implementation, we use free bits in the PTE to indicate whether a page is participating in COW. This helps us identify if the kernel is supposed to fix a write to a read only page when a page fault occurs. 

###### Ref-counting

There are complications to these new designs. 

One straightforward new necessity is ref counts for each of the physical frames.


When we are resolving COW page faults, we need to be aware of how many virtual pages are referencing it. 
- If there's only one virtual page referencing it, we can change its permission back to read-write from read-only. This is because the page is no longer shared with other COW pages.
- This ref count changes when we service COW page faults. For example, when we do a write to a read-only cow page, we allocate a new frame and copy the page over. The old page will see its refcount drop from 2 to 1. 

Of course, there are still many small details to the problem that are not specified here. 


###### Frame requests

Another elusive problem is the idea of frame requests. We did not have this in our original implementation attempt which resulted in a wrong implementation at first. 

Implementing COW as above introduces a new problem: the kernel may give processes the wrong "illusion" that it has enough space for everyone.

- Since when we first fork a new process and "share" pages, a big majority of the new frames are not being reserved for the current process.
- Many of such processes can be created without actually reserving enough frames for them, if they were to start writing to the pages.

This results in a serious correctness issue where the kernel is in some way "overpromising" memory for processes. 

This issue however can be somewhat easily solved by introducing a concept of frame requesting. When a child process is forked, we reserve some frames for it in advance, in the case that it writes to the frames. 



##### Reflections

These are solely part of some design reflections amongst others. Context switching for example, was also a huge slice of the cake, and also of the modules that were the most exciting and satisfying. 

Looking back, sitting down at a blank project folder, slowly creating everything module by module and thinking about how each interacts with another was probably one of the best technical learning experiences from this project. 

Rather than having a set of instructions to follow on how to implement the kernel, I think nothing beats having this blank slate to explore and discover for ourselves, making mistakes and rewriting code. 

These design decisions may not exactly be ideal and may have many loopholes here and there, but I think this process made me learn how to approach our own ideas critically, rigorously test and try to break them and build them to what we think we want them to be. 

:) 

<br>
