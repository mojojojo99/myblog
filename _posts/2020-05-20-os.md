---
title: "The happy little accidents"
last_modified_at: 2020-05-21T16:20:02-05:00
categories:
  - Coding
tags:
  - Operating Systems
header:
  teaser: /assets/images/teaser-post1.jpg
  overlay_image: /assets/images/teaser-post1.jpg
  overlay_filter: 0.5
---

### Part II of Operating Systems (15-410) and my experience

<br>

*Hmmm... That's not right...Uh-oh* 
 --- The so-called three types of errors 

<br>

In what you might call a deliberate rashness, my partner and I took the OS class (15-410) class, overloading it together with other CS classes like 15-451 and 15-259 (and other classes) in sophomore spring. 

This continues the previous post with a discussion on some of the more techincal learning points of the kernel project. (There are simply too many lessons learnt so other lessons will have to wait for perhaps a third post)

<br>

<center>............................................</center>


<br>

#### Interesting Detours

I'd like to talk about some of our thought processes when we designed our kernel. These design decisions may not be the best, but I think it's perhaps comic to document some of our imperfect approaches in a time capsule, dig it up a few years down the road and joke at the folly things we do.

<br>

###### Thread Control Blocks

One of the core abstractions of the kernel is the thread and process concepts. Although it seems pretty straightforward how a layer of indirection (threads and processes) can easily help to bridge the gaps between hardware and user, I think it's already applaudable how we arrived at these clean and neat idea of "task" abstraction that exactly nailed down the exact idea of a "thing" to manage space, time slices, resources etc. 

Standing the shoulders of giants, we now *just* have to find a way to implement and realise these abstractions.

Firstly, we used a "thread control block" (tcb) to represent the concept of a "kernel thread" and a "process control block" (pcb) to represent the concept of a "kernel process". 

The blocks are used to store important identification information we need for each of the tasks like their ids, variable_queue links for traversal, current state information (blocked, running, runnable etc.). 

One seemingly elementary problem that we encountered was where to store these blocks. Might seem straightforward but after some thought, we decided to place this tcb struct in the kernel thread's stack and have the pcb structs malloc-ed. 

One main driving reason behind this is the use case of each of these control blocks. When a trap to kernel space is triggered, we have immediately access to the kernel thread stack or at least some semblance of where and hence who we are. If we can get access to our threads, we can simply store a pointer to the process block in each of the thread control blocks.

```
 _______________
|               | <- Top of kernel thread stack
|               |
|               |
|               |
|               |
|_______________|
|      tcb      |
|_______________| <- Bottom of kernel thread stack

```

Since we have that the thread stacks are all thread stack size aligned, we can easily find the tcb of the current kernel thread from the current thread's esp. This allows us to do gettid and other thread related syscalls, for example, in O(1) time. Besides, this also allows us to bypass having to malloc an additional structure size when we create each new kernel thread; since malloc is expensive, this is much more time efficient too. There is very little cost to this implementation since the same amount of space is used. One disadvantage of this is that this structure may easily be corrupted if the kernel threads used more than the allocated stack space. 

We decided on this design ultimately because this small disadvantage can be avoided by more careful use of the stack in kernel mode, in exchange for quite a huge improvement in time and space. To check that our current syscall implementations will stay within in the stack space allocated, we also added canaries in the thread stack, and check frequently whether they are overwritten due to the overflowing stack.

<br>

###### Process wait and exit

One of the more tricky designs was with regards to the process wait and exit.

An alternative design (approach A) that we initially implemented and
considered was having waiting status flag in each parent thread tcb to
indicate whether a parent was waiting, received an exit signal already or was
not waiting. When a child is exiting, we looped through all the list of parent
threads for the exiting child, check and automatically set their status to
"received exit signal" if the parent was waiting.

We compare this with our current design (approach B), which is to instead have
another list of waiting threads for each pcb struct. In this case, we only have
to try to pop one parent off the queue when the child is trying to exit.


+-----------------+-------------------------------+-------------------------+
| Criteria        | Approach A                    | Approach B              |
+-----------------+-------------------------------+-------------------------+
| Time complexity | O(num of parent threads)      | O(1). Simply pop one    |
|                 | since we have to look         | element off the queue.  |
|                 | through each of the parent    |                         |
|                 | threads to check their status |                         |
+-----------------+-------------------------------+-------------------------+
| Locking         | Have to hold the lock         | Holds the lock when we  |
|                 | protecting the list of the    | pop off one element, a  |
|                 | thread lists while looping    | much shorter amount of  |
|                 | through.                      | time                    |
|                 |                               |                         |
|                 | This lock may potentially be  |                         |
|                 | highly contended for if the   |                         |
|                 | parent has many threads, and  |                         |
|                 | we are holding on to it for   |                         |
|                 | quite a long time.            |                         |
|                 |                               |                         |
|                 | There are further             |                         |
|                 | complications with respect    |                         |
|                 | to updating the parent's      |                         |
|                 | waiting status which requires |                         |
|                 | for us to hold on to a        |                         |
|                 | second lock while holding on  |                         |
|                 | to the lock above.            |                         |
+-----------------+-------------------------------+-------------------------+
| Space           | Little extra space since we   | Needs to store          |
|                 | only need an extra waiting    | waiting list related    |
|                 | status. The thread list is    | queue head              |
|                 | already included in the pcb.  |                         |
+-----------------+-------------------------------+-------------------------+
